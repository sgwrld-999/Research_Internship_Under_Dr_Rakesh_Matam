# Updated Algorithm: Temporal-Weighted K1K2NN Prediction (with mathematical refinements, compute optimizations, and robustness reparameterizations)

```markdown
Algorithm: Temporal-Weighted K1K2NN Prediction (Updated)

Function Predict(
    S,                // Training set {(x_i, Y_i, t_i)} for i=1...m
    x_q, t_q,         // Query feature vector and timestamp
    k1, k2,           // #neighbors in K1 and K2
    Œª,                // Exponential decay rate (used if half-life h not provided)
    Options           // Optional, see below
) -> Y_q

Inputs:
  S = {(x_i ‚àà ‚Ñù^d, Y_i ‚äÜ ùìõ, t_i)}_{i=1}^m
  x_q ‚àà ‚Ñù^d, t_q ‚àà ‚Ñù
  k1, k2 ‚àà ‚Ñï
  Œª ‚â• 0

Options (all optional; defaults keep original behavior):
  // Mathematical refinements
  - h                // half-life; if provided, Œª_eff := ln(2)/h; else Œª_eff := Œª
  - time_scale s     // robust time scale (e.g., MAD or IQR of Œît); default s := 1
  - Œ± ‚àà [0,1]        // Laplace smoothing for Jaccard; default Œ± := 0 (plain Jaccard)
  - Œ≤ ‚àà [0,2]        // distance exponent for weighted voting; default Œ≤ := 0 (majority)
  - œÑ ‚â• 0            // soft-vote threshold; if unset, use majority vote
  - Œ∏ ‚àà [0,1]        // min label-similarity for K2 (pre-filter); default: none
  - Œµ > 0            // small constant for numerical stability in logs/divisions

  // Preprocessing / feature-space refinement (no new stage; one-time offline)
  - Standardize: z-score with (Œº, œÉ) estimated on S
  - Whitening: diagonal Mahalanobis via œÉ (already implied by z-score)
  - PCA matrix P (retain 95‚Äì99% variance), applied once offline; default: identity

  // Compute optimizations (no logic change)
  - CacheSquaredNorms: store ||z_i'||^2
  - PrecomputeTopK2: for each label ‚Ñì, store top-K2 similar labels under smoothed Jaccard
  - UsePartialSelection: nth_element/heap instead of full sort for top-k1

Output:
  Y_q ‚äÜ ùìõ

-----------------------------------------
1. // ===== One-time OFFLINE PRECOMPUTATION =====
2. // Feature standardization / whitening (if not already z-scored)
3. Estimate Œº, œÉ on {x_i}; define z_i := (x_i - Œº) / œÉ  (elementwise);   // W = diag(1/œÉ^2)
4. If PCA P is provided: z_i' := P·µÄ z_i else z_i' := z_i
5. Cache ||z_i'||^2 for all i  (if CacheSquaredNorms)

6. // Label co-occurrence and similarity with Laplace smoothing
7. C ‚Üê ComputeLabelCooccurrenceCounts(S)   // counts C_{ij}, label marginals n_i, n_j
8. For all label pairs (i,j):
9.     J^(Œ±)(i,j) := (C_{ij} + Œ±) / ( (n_i + n_j - C_{ij}) + Œ± )   // Œ±=0 gives plain Jaccard
10. Sim_L ‚Üê { J^(Œ±)(i,j) }  // label‚Äìlabel similarity matrix

11. // (Optional) Precompute top-K2 labels per label for fast K2 expansion
12. If PrecomputeTopK2:
13.     For each label ‚Ñì:
14.         SimList[‚Ñì] := labels sorted by J^(Œ±)(‚Ñì, ¬∑) in descending order

15. // Robust time scaling and temporal decay
16. If time_scale s not provided: set s := 1   // or compute robust scale (e.g., MAD of Œît on S)
17. If half-life h provided: Œª_eff := ln(2)/h else Œª_eff := Œª

-----------------------------------------
18. // ===== PREDICTION PHASE =====
19. // Transform query to the same feature space
20. z_q := (x_q - Œº) / œÉ
21. If PCA P is provided: z_q' := P·µÄ z_q else z_q' := z_q
22. q2 := ||z_q'||^2

23. // Compute temporally weighted ranking score WITHOUT square root
24. DistScores ‚Üê empty list
25. For i = 1..m:
26.     // Squared Euclidean via inner products (BLAS-friendly)
27.     d2_i := q2 + ||z_i'||^2 - 2 * (z_q'·µÄ z_i')               // ‚â• 0
28.     Œît_i := max(0, t_q - t_i)                                // enforce non-negativity
29.     // Ranking-equivalent log-sum score (monotone in d2 and Œît)
30.     S_i := 0.5 * log(d2_i + Œµ) + Œª_eff * (Œît_i / s)
31.     Append (S_i, i, d2_i, Œît_i) to DistScores
32. End For

33. // Select top-k1 neighbors by PARTIAL SELECTION (no full sort)
34. K1_Idx ‚Üê SelectTopKByScore(DistScores, k1)   // e.g., nth_element/heap on S_i ascending

35. // ===== K1 label aggregation (original majority or optional soft weighted vote) =====
36. If œÑ is unset (default):   // ORIGINAL MAJORITY
37.     Y_q_initial := MajorityVote({Y_i | i ‚àà K1_Idx})          // include label if count > k1/2
38. Else:                    // OPTIONAL SOFT VOTE using distance & time weights
39.     For each label ‚Ñì ‚àà ùìõ:
40.         vÃÇ(‚Ñì) := Œ£_{i ‚àà K1_Idx} [ exp(-Œª_eff * (Œît_i / s)) / (d2_i + Œµ)^(Œ≤/2) ] * ùüô[‚Ñì ‚àà Y_i]
41.     Y_q_initial := { ‚Ñì : vÃÇ(‚Ñì) ‚â• œÑ }
42.     // Optional deterministic tie-break (when needed): prefer labels with larger
43.     // Œ£_{‚Ñì' ‚àà Y_q_initial \ {‚Ñì}} J^(Œ±)(‚Ñì, ‚Ñì').

44. // ===== K2 label expansion (same step, smoothed similarity, optional threshold) =====
45. Y_q_expansion := ‚àÖ
46. For each label ‚Ñì_a ‚àà Y_q_initial:
47.     if PrecomputeTopK2:
48.         Candidates := SimList[‚Ñì_a]            // already sorted by J^(Œ±)
49.     else:
50.         Candidates := all labels sorted by J^(Œ±)(‚Ñì_a, ¬∑) in descending order
51.     if Œ∏ is provided:
52.         Candidates := { ‚Ñì : ‚Ñì ‚àà Candidates and J^(Œ±)(‚Ñì_a, ‚Ñì) ‚â• Œ∏ }
53.     K2_Labels := first k2 elements of Candidates
54.     Y_q_expansion := Y_q_expansion ‚à™ K2_Labels
55. End For

56. // Final prediction
57. Y_q := Y_q_initial ‚à™ Y_q_expansion
58. return Y_q
```

### Notes (concise, implementation-oriented)

* **Monotone temporal penalization:** The effective distance is ( |z_q' - z_i'|*2 \cdot \exp(\lambda*{\text{eff}} \Delta t_i / s) ). We rank by the equivalent **log-score** ( S_i = \tfrac12 \log(d2_i + \varepsilon) + \lambda_{\text{eff}} \Delta t_i / s ), which avoids square roots and improves numerical stability.
* **Whitening / standardization:** Using z-scores (or diagonal Mahalanobis) is a reparameterization of the same K1 step; it reduces scale-induced bias in IoT features (counts/bytes/flags).
* **Smoothed Jaccard (K2):** ( J^{(\alpha)}(i,j) = \frac{C_{ij} + \alpha}{(n_i + n_j - C_{ij}) + \alpha} ) stabilizes similarity for rare labels without changing the K2 mechanism. Set ( \alpha=0 ) to recover the original.
* **Half-life parameterization:** If provided, ( \lambda_{\text{eff}} = \ln 2 / h ); this keeps the same temporal mechanism but makes Œª interpretable operationally.
* **Partial selection:** Use `SelectTopKByScore` (nth_element/heap) to get k‚ÇÅ neighbors in linear-time average, rather than sorting all m distances.
* **Weighted voting (optional):** Lines 39‚Äì41 keep aggregation inside the existing K1 step. If you prefer the original behavior, skip œÑ (lines 36‚Äì41) and the algorithm reverts to majority voting.
* **PCA (optional):** A one-time linear projection for denoising/acceleration; it does not add a new stage‚ÄîK1 still runs in the (transformed) feature space.
* **Caching:** Precompute `||z_i'||^2`, co-occurrence counts, `J^(Œ±)`, and, if enabled, per-label top-K2 lists to reduce per-query latency.
