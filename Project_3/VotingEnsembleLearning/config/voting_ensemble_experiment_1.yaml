# Voting Ensemble Model Configuration for Network Intrusion Detection
# ====================================================================
# 
# THEORY - Configuration Design:
# =============================
# This configuration follows the principle of "Configuration as Code":
# - All model parameters are externalized
# - Easy to version control and track changes
# - Enables experiment reproducibility
# - Supports automated hyperparameter tuning

# === Core Architecture Parameters ===
input_dim: 14 # Number of features per sample (update to match dataset)
num_classes: 5 # Classification categories: Recon, Exploitation, C&C, Attack, Benign

# === Ensemble Configuration ===
voting_type: "soft" # Use soft voting for better probability estimates
estimator_weights: null # Equal weights for all estimators (null = automatic)
n_jobs: -1 # Use all available CPU cores for parallel processing

# === Base Estimators Selection ===
use_random_forest: true # Include Random Forest classifier
use_svm: false # Include Support Vector Machine(optional)
use_logistic_regression: true # Include Logistic Regression
use_gradient_boosting: true # Include Gradient Boosting
use_knn: true # Include K-Nearest Neighbors (optional)
use_naive_bayes: true # Include Naive Bayes (optional)
use_decision_tree: false # Include Decision Tree (optional)
use_xgboost: true # Include XGBoost (optional)
use_lightgbm: true # Include LightGBM (optional)

# === Random Forest Parameters ===
rf_n_estimators: 100 # Number of trees in Random Forest
rf_max_depth: null # Maximum depth (null for unlimited)
rf_random_state: 42 # Random seed for Random Forest

# === SVM Parameters ===
svm_kernel: "rbf" # Kernel type for SVM
svm_c: 1.0 # Regularization parameter
svm_gamma: "scale" # Kernel coefficient
svm_probability: true # Enable probability estimates (required for soft voting)

# === Logistic Regression Parameters ===
lr_c: 1.0 # Inverse of regularization strength
lr_max_iter: 1000 # Maximum iterations for convergence
lr_solver: "lbfgs" # Optimization algorithm

# === Gradient Boosting Parameters ===
gb_n_estimators: 100 # Number of boosting stages
gb_learning_rate: 0.1 # Learning rate for gradient boosting
gb_max_depth: 3 # Maximum depth of individual trees

# === K-Nearest Neighbors Parameters ===
knn_n_neighbors: 5 # Number of neighbors to consider
knn_weights: "uniform" # Weight function used in prediction
knn_algorithm: "auto" # Algorithm to compute nearest neighbors
knn_n_jobs: -1 # Use all CPU cores for KNN

# === Naive Bayes Parameters ===
nb_var_smoothing: 1e-9 # Portion of the largest variance to add to variances for stability
nb_priors: null # Class prior probabilities (null = learned from data)
# nb_fit_prior: true # Whether to learn class prior probabilities
# nb_alpha: 1.0 # Additive (Laplace) smoothing parameter
# nb_binarize: null # Threshold for binarizing (null = no binarization)
# nb_class_prior: null # Prior probabilities of the classes (null = uniform)

# === XGBoost Parameters ===
xgb_n_estimators: 100 # Number of boosting rounds
xgb_learning_rate: 0.1 # Learning rate
xgb_max_depth: 3 # Maximum tree depth for base learners
xgb_subsample: 1.0 # Subsample ratio of the training instances
xgb_colsample_bytree: 1.0 # Subsample ratio of columns when constructing each tree
xgb_random_state: 42 # Random seed for XGBoost
xgb_use_label_encoder: false # Disable label encoder for multi-class classification
xgb_eval_metric: "mlogloss" # Evaluation metric for multi-class classification
# xgb_gamma: 0 # Minimum loss reduction required to make a further partition on a leaf node
# xgb_min_child_weight: 1 # Minimum sum of instance weight needed in a child
# xgb_reg_alpha: 0 # L1 regularization term on weights
# xgb_reg_lambda: 1 # L2 regularization term on weights

# === Decision Tree Parameters ===
dt_criterion: "gini" # Function to measure the quality of a split
dt_max_depth: null # Maximum depth of the tree (null = unlimited)
dt_min_samples_split: 2 # Minimum samples required to split an internal node
dt_random_state: 42 # Random seed for Decision Tree
# dt_min_samples_leaf: 1 # Minimum samples required to be at a leaf node
# dt_max_features: null # Number of features to consider when looking for the best split
# dt_class_weight: null # Weights associated with classes (null = uniform)
# dt_splitter: "best" # Strategy used to choose the split at each node

# === LightGBM Parameters ===
lgbm_n_estimators: 100 # Number of boosting rounds
lgbm_learning_rate: 0.1 # Learning rate
lgbm_max_depth: -1 # Maximum tree depth for base learners (-1 = unlimited)
lgbm_num_leaves: 31 # Maximum number of leaves in one tree
lgbm_subsample: 1.0 # Subsample ratio of the training instances
lgbm_colsample_bytree: 0.5 # Subsample ratio of columns when constructing each tree
lgbm_random_state: 42 # Random seed for LightGBM
lgbm_use_label_encoder: false # Disable label encoder for multi-class classification
lgbm_eval_metric: "multi_logloss" # Evaluation metric for multi-class classification
# lgbm_min_child_samples: 20 # Minimum number of data needed in a child (leaf)
# lgbm_reg_alpha: 0 # L1 regularization term on weights
# lgbm_reg_lambda: 0 # L2 regularization term on weights




# === Training Configuration ===
test_size: 0.2 # Fraction of data for testing (20%)
validation_split: 0.2 # Fraction of training data for validation (20%)
random_state: 42 # Global random seed for reproducibility

# === Evaluation Metrics ===
metrics: # Performance metrics to track
  - accuracy # Overall correctness
  - precision # Positive predictive value
  - recall # Sensitivity to positive class
  - f1_score # Balance between precision and recall

# === Model Persistence ===
export_path: "models/saved_Models/voting_ensemble_experiment1.joblib" # Where to save trained model

# === Advanced Configuration (Optional) ===
# Uncomment and modify these for advanced usage:
# 
# estimator_weights: [0.3, 0.3, 0.2, 0.2] # Custom weights for [RF, SVM, LR, GB]
# 
# feature_engineering:
#   feature_selection: true
#   selection_method: "mutual_info"
#   n_features: 10

# === GPU/CUDA Acceleration ===
use_gpu: true # Set to true to enable GPU acceleration for supported models
device: "cuda" # "cuda" for NVIDIA, "cpu" for CPU
