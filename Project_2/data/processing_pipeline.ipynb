{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b0928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Datasets/CIC_IoT/ciciot_processed_datasets_corrected/test_50_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f70e195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>ack_flag_number</th>\n",
       "      <th>ece_flag_number</th>\n",
       "      <th>cwr_flag_number</th>\n",
       "      <th>ack_count</th>\n",
       "      <th>syn_count</th>\n",
       "      <th>fin_count</th>\n",
       "      <th>urg_count</th>\n",
       "      <th>rst_count</th>\n",
       "      <th>HTTP</th>\n",
       "      <th>HTTPS</th>\n",
       "      <th>DNS</th>\n",
       "      <th>Telnet</th>\n",
       "      <th>SMTP</th>\n",
       "      <th>SSH</th>\n",
       "      <th>IRC</th>\n",
       "      <th>TCP</th>\n",
       "      <th>UDP</th>\n",
       "      <th>DHCP</th>\n",
       "      <th>ARP</th>\n",
       "      <th>ICMP</th>\n",
       "      <th>IPv</th>\n",
       "      <th>LLC</th>\n",
       "      <th>Tot sum</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>AVG</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.097389</td>\n",
       "      <td>15757.58</td>\n",
       "      <td>6.22</td>\n",
       "      <td>65.91</td>\n",
       "      <td>188.594810</td>\n",
       "      <td>188.594810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4.26</td>\n",
       "      <td>13.17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14116.33</td>\n",
       "      <td>474.38</td>\n",
       "      <td>2776.48</td>\n",
       "      <td>1439.655500</td>\n",
       "      <td>811.963557</td>\n",
       "      <td>1184.84</td>\n",
       "      <td>8.299881e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>53.007713</td>\n",
       "      <td>1148.986652</td>\n",
       "      <td>7.820311e+05</td>\n",
       "      <td>0.91</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-HTTP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.87</td>\n",
       "      <td>45.93</td>\n",
       "      <td>67.18</td>\n",
       "      <td>5.430061</td>\n",
       "      <td>5.430061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6097.63</td>\n",
       "      <td>473.63</td>\n",
       "      <td>592.00</td>\n",
       "      <td>583.338703</td>\n",
       "      <td>30.364513</td>\n",
       "      <td>577.71</td>\n",
       "      <td>8.370265e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>34.151703</td>\n",
       "      <td>43.050064</td>\n",
       "      <td>4.164176e+03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>141.55</td>\n",
       "      <td>Mirai-greeth_flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.942609</td>\n",
       "      <td>2096427.60</td>\n",
       "      <td>6.00</td>\n",
       "      <td>87.60</td>\n",
       "      <td>708.744515</td>\n",
       "      <td>708.744515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.60</td>\n",
       "      <td>1376.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11810.20</td>\n",
       "      <td>903.60</td>\n",
       "      <td>3974.80</td>\n",
       "      <td>2296.940952</td>\n",
       "      <td>1117.249136</td>\n",
       "      <td>2020.40</td>\n",
       "      <td>1.711845e-04</td>\n",
       "      <td>5.5</td>\n",
       "      <td>67.514768</td>\n",
       "      <td>1580.028881</td>\n",
       "      <td>1.831754e+06</td>\n",
       "      <td>0.70</td>\n",
       "      <td>38.50</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.501263</td>\n",
       "      <td>194091.30</td>\n",
       "      <td>7.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>24.974217</td>\n",
       "      <td>24.974217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>312.10</td>\n",
       "      <td>732.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6936.80</td>\n",
       "      <td>60.00</td>\n",
       "      <td>2763.00</td>\n",
       "      <td>459.858260</td>\n",
       "      <td>778.392523</td>\n",
       "      <td>102.80</td>\n",
       "      <td>1.665218e+08</td>\n",
       "      <td>13.5</td>\n",
       "      <td>30.305189</td>\n",
       "      <td>1102.864634</td>\n",
       "      <td>6.114278e+05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>244.60</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.460552</td>\n",
       "      <td>472795.40</td>\n",
       "      <td>6.00</td>\n",
       "      <td>128.60</td>\n",
       "      <td>54.081306</td>\n",
       "      <td>54.081306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.50</td>\n",
       "      <td>454.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12179.80</td>\n",
       "      <td>66.00</td>\n",
       "      <td>1514.00</td>\n",
       "      <td>809.852264</td>\n",
       "      <td>604.794938</td>\n",
       "      <td>115.90</td>\n",
       "      <td>1.665207e+08</td>\n",
       "      <td>13.5</td>\n",
       "      <td>40.198486</td>\n",
       "      <td>855.591058</td>\n",
       "      <td>3.664233e+05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>244.60</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flow_duration  Header_Length  Protocol Type  Duration        Rate  \\\n",
       "0       5.097389       15757.58           6.22     65.91  188.594810   \n",
       "1       0.000000           2.87          45.93     67.18    5.430061   \n",
       "2       1.942609     2096427.60           6.00     87.60  708.744515   \n",
       "3      59.501263      194091.30           7.70    104.70   24.974217   \n",
       "4       8.460552      472795.40           6.00    128.60   54.081306   \n",
       "\n",
       "        Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  \\\n",
       "0  188.594810    0.0              0.0              0.0              0.0   \n",
       "1    5.430061    0.0              0.0              0.0              0.0   \n",
       "2  708.744515    0.0              0.0              0.0              0.0   \n",
       "3   24.974217    0.0              0.0              0.0              0.0   \n",
       "4   54.081306    0.0              0.0              0.0              0.0   \n",
       "\n",
       "   psh_flag_number  ack_flag_number  ece_flag_number  cwr_flag_number  \\\n",
       "0              0.0              1.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              1.0              0.0              0.0   \n",
       "3              0.0              1.0              0.0              0.0   \n",
       "4              0.0              1.0              0.0              0.0   \n",
       "\n",
       "   ack_count  syn_count  fin_count  urg_count  rst_count  HTTP  HTTPS  DNS  \\\n",
       "0       0.32       0.65       0.54       4.26      13.17   1.0    0.0  0.0   \n",
       "1       0.00       0.00       0.00       0.00       0.00   0.0    0.0  0.0   \n",
       "2       0.00       0.00       0.00      17.60    1376.80   1.0    0.0  0.0   \n",
       "3       0.00       0.20       0.00     312.10     732.00   0.0    1.0  0.0   \n",
       "4       0.00       0.00       0.00      31.50     454.10   0.0    0.0  0.0   \n",
       "\n",
       "   Telnet  SMTP  SSH  IRC  TCP  UDP  DHCP  ARP  ICMP  IPv  LLC   Tot sum  \\\n",
       "0     0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0   0.0  1.0  1.0  14116.33   \n",
       "1     0.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  1.0  1.0   6097.63   \n",
       "2     0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0   0.0  1.0  1.0  11810.20   \n",
       "3     0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0   0.0  1.0  1.0   6936.80   \n",
       "4     0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0   0.0  1.0  1.0  12179.80   \n",
       "\n",
       "      Min      Max          AVG          Std  Tot size           IAT  Number  \\\n",
       "0  474.38  2776.48  1439.655500   811.963557   1184.84  8.299881e+07     9.5   \n",
       "1  473.63   592.00   583.338703    30.364513    577.71  8.370265e+07     9.5   \n",
       "2  903.60  3974.80  2296.940952  1117.249136   2020.40  1.711845e-04     5.5   \n",
       "3   60.00  2763.00   459.858260   778.392523    102.80  1.665218e+08    13.5   \n",
       "4   66.00  1514.00   809.852264   604.794938    115.90  1.665207e+08    13.5   \n",
       "\n",
       "    Magnitue       Radius    Covariance  Variance  Weight               label  \n",
       "0  53.007713  1148.986652  7.820311e+05      0.91  141.55      DoS-HTTP_Flood  \n",
       "1  34.151703    43.050064  4.164176e+03      0.23  141.55  Mirai-greeth_flood  \n",
       "2  67.514768  1580.028881  1.831754e+06      0.70   38.50       BenignTraffic  \n",
       "3  30.305189  1102.864634  6.114278e+05      1.00  244.60       BenignTraffic  \n",
       "4  40.198486   855.591058  3.664233e+05      1.00  244.60       BenignTraffic  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c07b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1139511 entries, 0 to 1139510\n",
      "Data columns (total 47 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   flow_duration    1139511 non-null  float64\n",
      " 1   Header_Length    1139511 non-null  float64\n",
      " 2   Protocol Type    1139511 non-null  float64\n",
      " 3   Duration         1139511 non-null  float64\n",
      " 4   Rate             1139511 non-null  float64\n",
      " 5   Srate            1139511 non-null  float64\n",
      " 6   Drate            1139511 non-null  float64\n",
      " 7   fin_flag_number  1139511 non-null  float64\n",
      " 8   syn_flag_number  1139511 non-null  float64\n",
      " 9   rst_flag_number  1139511 non-null  float64\n",
      " 10  psh_flag_number  1139511 non-null  float64\n",
      " 11  ack_flag_number  1139511 non-null  float64\n",
      " 12  ece_flag_number  1139511 non-null  float64\n",
      " 13  cwr_flag_number  1139511 non-null  float64\n",
      " 14  ack_count        1139511 non-null  float64\n",
      " 15  syn_count        1139511 non-null  float64\n",
      " 16  fin_count        1139511 non-null  float64\n",
      " 17  urg_count        1139511 non-null  float64\n",
      " 18  rst_count        1139511 non-null  float64\n",
      " 19  HTTP             1139511 non-null  float64\n",
      " 20  HTTPS            1139511 non-null  float64\n",
      " 21  DNS              1139511 non-null  float64\n",
      " 22  Telnet           1139511 non-null  float64\n",
      " 23  SMTP             1139511 non-null  float64\n",
      " 24  SSH              1139511 non-null  float64\n",
      " 25  IRC              1139511 non-null  float64\n",
      " 26  TCP              1139511 non-null  float64\n",
      " 27  UDP              1139511 non-null  float64\n",
      " 28  DHCP             1139511 non-null  float64\n",
      " 29  ARP              1139511 non-null  float64\n",
      " 30  ICMP             1139511 non-null  float64\n",
      " 31  IPv              1139511 non-null  float64\n",
      " 32  LLC              1139511 non-null  float64\n",
      " 33  Tot sum          1139511 non-null  float64\n",
      " 34  Min              1139511 non-null  float64\n",
      " 35  Max              1139511 non-null  float64\n",
      " 36  AVG              1139511 non-null  float64\n",
      " 37  Std              1139511 non-null  float64\n",
      " 38  Tot size         1139511 non-null  float64\n",
      " 39  IAT              1139511 non-null  float64\n",
      " 40  Number           1139511 non-null  float64\n",
      " 41  Magnitue         1139511 non-null  float64\n",
      " 42  Radius           1139511 non-null  float64\n",
      " 43  Covariance       1139511 non-null  float64\n",
      " 44  Variance         1139511 non-null  float64\n",
      " 45  Weight           1139511 non-null  float64\n",
      " 46  label            1139511 non-null  object \n",
      "dtypes: float64(46), object(1)\n",
      "memory usage: 408.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Selecting features...\n",
      "Encoding labels...\n",
      "Scaling features...\n",
      "Splitting data...\n",
      "Saved train_data.csv with shape (683706, 41)\n",
      "Saved val_data.csv with shape (227902, 41)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CICIoTDataPipeline:\n",
    "    \"\"\"Data processing pipeline for CIC-IoT-2023 dataset for GRIFFIN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path: str,\n",
    "                 output_dir: str = '.',\n",
    "                 test_size: float = 0.2,\n",
    "                 val_size: float = 0.2,\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the raw CIC-IoT dataset CSV file\n",
    "            output_dir: Directory to save processed data\n",
    "            test_size: Proportion of data for testing\n",
    "            val_size: Proportion of training data for validation\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Feature groups for GRIFFIN (Protocol-Aware Group Gates)\n",
    "        self.feature_groups = {\n",
    "            'packet_stats': [\n",
    "                'Total Fwd Packets', 'Total Backward Packets',\n",
    "                'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "                'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "                'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
    "                'Bwd Packet Length Mean', 'Bwd Packet Length Std'\n",
    "            ],\n",
    "            'time_features': [\n",
    "                'Flow Duration', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "                'Flow IAT Max', 'Flow IAT Min',\n",
    "                'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
    "                'Bwd IAT Total', 'Bwd IAT Mean'\n",
    "            ],\n",
    "            'flow_rates': [\n",
    "                'Flow Bytes/s', 'Flow Packets/s',\n",
    "                'Fwd Packets/s', 'Bwd Packets/s',\n",
    "                'Packet Length Mean', 'Packet Length Std',\n",
    "                'Packet Length Variance', 'Down/Up Ratio',\n",
    "                'Average Packet Size', 'Fwd Segment Size Avg'\n",
    "            ],\n",
    "            'tcp_flags': [\n",
    "                'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count',\n",
    "                'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count',\n",
    "                'CWE Flag Count', 'ECE Flag Count',\n",
    "                'Fwd PSH Flags', 'Bwd PSH Flags'\n",
    "            ],\n",
    "            'protocol_info': [\n",
    "                'Protocol', 'Source Port', 'Destination Port',\n",
    "                'Init Win bytes forward', 'Init Win bytes backward',\n",
    "                'Active Mean', 'Active Std', 'Active Max',\n",
    "                'Idle Mean', 'Idle Std'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.selected_features = None\n",
    "        self.feature_names = None\n",
    "        self.class_weights = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_and_clean_data(self, df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and perform initial cleaning of the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: Optional DataFrame if data is already loaded\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        \n",
    "        # Store original shape\n",
    "        original_shape = df.shape\n",
    "        \n",
    "        # Handle column names (remove spaces, convert to lowercase)\n",
    "        df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('/', '_')\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Handle infinity values\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Clip extreme values\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].clip(-1e10, 1e10)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "        \n",
    "        # Remove constant features (variance < 1e-6)\n",
    "        constant_features = []\n",
    "        for col in numeric_columns:\n",
    "            if df[col].var() < 1e-6:\n",
    "                constant_features.append(col)\n",
    "        \n",
    "        if constant_features:\n",
    "            df = df.drop(columns=constant_features)\n",
    "        \n",
    "        self.metadata['original_shape'] = original_shape\n",
    "        self.metadata['cleaned_shape'] = df.shape\n",
    "        self.metadata['removed_constants'] = constant_features\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def select_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Select and organize features according to GRIFFIN groups.\n",
    "        \n",
    "        Args:\n",
    "            df: Cleaned DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with selected features\n",
    "        \"\"\"\n",
    "        # Normalize column names for matching\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Create mapping for feature groups (normalized names)\n",
    "        normalized_groups = {}\n",
    "        for group_name, features in self.feature_groups.items():\n",
    "            normalized_groups[group_name] = [\n",
    "                f.lower().replace(' ', '').replace('/', '').replace('_', '')\n",
    "                for f in features\n",
    "            ]\n",
    "        \n",
    "        # Find matching columns\n",
    "        selected_cols = []\n",
    "        feature_to_group = {}\n",
    "        \n",
    "        for group_name, norm_features in normalized_groups.items():\n",
    "            for norm_feat in norm_features:\n",
    "                matching_cols = [col for col in df.columns \n",
    "                               if norm_feat in col or col in norm_feat]\n",
    "                if matching_cols:\n",
    "                    col = matching_cols[0]  # Take first match\n",
    "                    if col not in selected_cols and col != 'label':\n",
    "                        selected_cols.append(col)\n",
    "                        feature_to_group[col] = group_name\n",
    "        \n",
    "        # Ensure we have the label column\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower() or 'class' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col is None:\n",
    "            raise ValueError(\"No label column found in dataset\")\n",
    "        \n",
    "        # Select features and label\n",
    "        if len(selected_cols) < 20:  # Fallback if feature matching fails\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            selected_cols = [col for col in numeric_cols if col != label_col][:46]\n",
    "        \n",
    "        self.selected_features = selected_cols\n",
    "        self.feature_names = selected_cols\n",
    "        self.metadata['feature_groups'] = feature_to_group\n",
    "        self.metadata['num_features'] = len(selected_cols)\n",
    "        \n",
    "        return df[selected_cols + [label_col]]\n",
    "    \n",
    "    def encode_labels(self, y: pd.Series, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode labels to integers.\n",
    "        \n",
    "        Args:\n",
    "            y: Label series\n",
    "            fit: Whether to fit the encoder\n",
    "            \n",
    "        Returns:\n",
    "            Encoded labels\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            self.metadata['classes'] = self.label_encoder.classes_.tolist()\n",
    "            self.metadata['num_classes'] = len(self.label_encoder.classes_)\n",
    "            \n",
    "            # Calculate class weights for imbalanced learning\n",
    "            unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "            self.class_weights = len(y_encoded) / (len(unique) * counts)\n",
    "            self.metadata['class_distribution'] = dict(zip(\n",
    "                self.label_encoder.classes_,\n",
    "                counts.tolist()\n",
    "            ))\n",
    "        else:\n",
    "            y_encoded = self.label_encoder.transform(y)\n",
    "        \n",
    "        return y_encoded\n",
    "    \n",
    "    def scale_features(self, X: pd.DataFrame, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale features using StandardScaler.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            fit: Whether to fit the scaler\n",
    "            \n",
    "        Returns:\n",
    "            Scaled features\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Split data into train, validation, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature array\n",
    "            y: Label array\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train, val, test splits\n",
    "        \"\"\"\n",
    "        # First split: train+val vs test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size_adjusted = self.val_size / (1 - self.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=val_size_adjusted,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        splits = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        # Store split sizes in metadata\n",
    "        self.metadata['split_sizes'] = {\n",
    "            'train': len(X_train),\n",
    "            'val': len(X_val),\n",
    "            'test': len(X_test)\n",
    "        }\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def process(self, save: bool = True) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run the complete processing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            save: Whether to save processed data to disk\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed data splits\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        df = self.load_and_clean_data()\n",
    "        \n",
    "        print(\"Selecting features...\")\n",
    "        df = self.select_features(df)\n",
    "        \n",
    "        # Separate features and labels\n",
    "        label_col = df.columns[-1]\n",
    "        X = df.drop(columns=[label_col])\n",
    "        y = df[label_col]\n",
    "        \n",
    "        print(\"Encoding labels...\")\n",
    "        y_encoded = self.encode_labels(y, fit=True)\n",
    "        \n",
    "        print(\"Scaling features...\")\n",
    "        X_scaled = self.scale_features(X, fit=True)\n",
    "        \n",
    "        print(\"Splitting data...\")\n",
    "        splits = self.split_data(X_scaled, y_encoded)\n",
    "        \n",
    "        if save:\n",
    "            self.save_processed_data(splits)\n",
    "            self.save_preprocessors()\n",
    "            self.save_metadata()\n",
    "            self.save_combined_csv(splits)\n",
    "        \n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Train: {splits['X_train'].shape}, Val: {splits['X_val'].shape}, Test: {splits['X_test'].shape}\")\n",
    "        print(f\"Classes: {self.metadata['num_classes']}\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def save_combined_csv(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save combined feature-label CSV files for each split.\"\"\"\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            X = splits[f'X_{split}']\n",
    "            y = splits[f'y_{split}']\n",
    "            \n",
    "            # Create DataFrame with features and labels\n",
    "            df = pd.DataFrame(X, columns=self.feature_names)\n",
    "            df['label_encoded'] = y\n",
    "            df['label'] = self.label_encoder.inverse_transform(y)\n",
    "            \n",
    "            # Save combined CSV\n",
    "            df.to_csv(self.output_dir / f'{split}_data.csv', index=False)\n",
    "            print(f\"Saved {split}_data.csv with shape {df.shape}\")\n",
    "    \n",
    "    def save_processed_data(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save processed data splits to disk.\"\"\"\n",
    "        for name, data in splits.items():\n",
    "            np.save(self.output_dir / f'{name}.npy', data)\n",
    "    \n",
    "    def save_preprocessors(self):\n",
    "        \"\"\"Save fitted preprocessors.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            np.save(self.output_dir / 'class_weights.npy', self.class_weights)\n",
    "    \n",
    "    def save_metadata(self):\n",
    "        \"\"\"Save metadata as JSON.\"\"\"\n",
    "        metadata = self.metadata.copy()\n",
    "        metadata['feature_names'] = self.feature_names\n",
    "        metadata['selected_features'] = self.selected_features\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def load_preprocessors(self):\n",
    "        \"\"\"Load saved preprocessors for inference.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        if (self.output_dir / 'class_weights.npy').exists():\n",
    "            self.class_weights = np.load(self.output_dir / 'class_weights.npy')\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            self.feature_names = self.metadata.get('feature_names', [])\n",
    "            self.selected_features = self.metadata.get('selected_features', [])\n",
    "    \n",
    "    def transform_new_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Transform new data using fitted preprocessors.\n",
    "        \n",
    "        Args:\n",
    "            df: New data DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (scaled features, encoded labels or None)\n",
    "        \"\"\"\n",
    "        # Clean data\n",
    "        df = self.load_and_clean_data(df)\n",
    "        \n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Select features\n",
    "        selected_cols = []\n",
    "        for feat in self.selected_features:\n",
    "            matching_cols = [col for col in df.columns if feat in col or col in feat]\n",
    "            if matching_cols:\n",
    "                selected_cols.append(matching_cols[0])\n",
    "            else:\n",
    "                # Create missing column with zeros\n",
    "                df[feat] = 0\n",
    "                selected_cols.append(feat)\n",
    "        \n",
    "        X = df[selected_cols]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scale_features(X, fit=False)\n",
    "        \n",
    "        # Handle labels if present\n",
    "        y_encoded = None\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col:\n",
    "            y = df[label_col]\n",
    "            y_encoded = self.encode_labels(y, fit=False)\n",
    "        \n",
    "        return X_scaled, y_encoded\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = CICIoTDataPipeline(\n",
    "        data_path=\"/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Datasets/CIC_IoT/ciciot_processed_datasets_corrected/test_50_50.csv\",  # INSERT YOUR PATH HERE\n",
    "        output_dir=\"./processed_data\",\n",
    "        test_size=0.2,\n",
    "        val_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Process training data\n",
    "    splits = pipeline.process(save=True)\n",
    "    \n",
    "    # For inference on new data:\n",
    "    # pipeline.load_preprocessors()\n",
    "    # X_new, y_new = pipeline.transform_new_data(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CICIoTDataPipeline:\n",
    "    \"\"\"Data processing pipeline for CIC-IoT-2023 dataset for GRIFFIN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path: str,\n",
    "                 output_dir: str = '.',\n",
    "                 test_size: float = 0.2,\n",
    "                 val_size: float = 0.2,\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the raw CIC-IoT dataset CSV file\n",
    "            output_dir: Directory to save processed data\n",
    "            test_size: Proportion of data for testing\n",
    "            val_size: Proportion of training data for validation\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Feature groups for GRIFFIN (Protocol-Aware Group Gates)\n",
    "        self.feature_groups = {\n",
    "            'packet_stats': [\n",
    "                'Total Fwd Packets', 'Total Backward Packets',\n",
    "                'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "                'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "                'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
    "                'Bwd Packet Length Mean', 'Bwd Packet Length Std'\n",
    "            ],\n",
    "            'time_features': [\n",
    "                'Flow Duration', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "                'Flow IAT Max', 'Flow IAT Min',\n",
    "                'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
    "                'Bwd IAT Total', 'Bwd IAT Mean'\n",
    "            ],\n",
    "            'flow_rates': [\n",
    "                'Flow Bytes/s', 'Flow Packets/s',\n",
    "                'Fwd Packets/s', 'Bwd Packets/s',\n",
    "                'Packet Length Mean', 'Packet Length Std',\n",
    "                'Packet Length Variance', 'Down/Up Ratio',\n",
    "                'Average Packet Size', 'Fwd Segment Size Avg'\n",
    "            ],\n",
    "            'tcp_flags': [\n",
    "                'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count',\n",
    "                'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count',\n",
    "                'CWE Flag Count', 'ECE Flag Count',\n",
    "                'Fwd PSH Flags', 'Bwd PSH Flags'\n",
    "            ],\n",
    "            'protocol_info': [\n",
    "                'Protocol', 'Source Port', 'Destination Port',\n",
    "                'Init Win bytes forward', 'Init Win bytes backward',\n",
    "                'Active Mean', 'Active Std', 'Active Max',\n",
    "                'Idle Mean', 'Idle Std'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.selected_features = None\n",
    "        self.feature_names = None\n",
    "        self.class_weights = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_and_clean_data(self, df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and perform initial cleaning of the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: Optional DataFrame if data is already loaded\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        \n",
    "        # Store original shape\n",
    "        original_shape = df.shape\n",
    "        \n",
    "        # Handle column names (remove spaces, convert to lowercase)\n",
    "        df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('/', '_')\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Handle infinity values\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Clip extreme values\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].clip(-1e10, 1e10)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "        \n",
    "        # Remove constant features (variance < 1e-6)\n",
    "        constant_features = []\n",
    "        for col in numeric_columns:\n",
    "            if df[col].var() < 1e-6:\n",
    "                constant_features.append(col)\n",
    "        \n",
    "        if constant_features:\n",
    "            df = df.drop(columns=constant_features)\n",
    "        \n",
    "        self.metadata['original_shape'] = original_shape\n",
    "        self.metadata['cleaned_shape'] = df.shape\n",
    "        self.metadata['removed_constants'] = constant_features\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def select_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Select and organize features according to GRIFFIN groups.\n",
    "        \n",
    "        Args:\n",
    "            df: Cleaned DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with selected features\n",
    "        \"\"\"\n",
    "        # Normalize column names for matching\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Create mapping for feature groups (normalized names)\n",
    "        normalized_groups = {}\n",
    "        for group_name, features in self.feature_groups.items():\n",
    "            normalized_groups[group_name] = [\n",
    "                f.lower().replace(' ', '').replace('/', '').replace('_', '')\n",
    "                for f in features\n",
    "            ]\n",
    "        \n",
    "        # Find matching columns\n",
    "        selected_cols = []\n",
    "        feature_to_group = {}\n",
    "        \n",
    "        for group_name, norm_features in normalized_groups.items():\n",
    "            for norm_feat in norm_features:\n",
    "                matching_cols = [col for col in df.columns \n",
    "                               if norm_feat in col or col in norm_feat]\n",
    "                if matching_cols:\n",
    "                    col = matching_cols[0]  # Take first match\n",
    "                    if col not in selected_cols and col != 'label':\n",
    "                        selected_cols.append(col)\n",
    "                        feature_to_group[col] = group_name\n",
    "        \n",
    "        # Ensure we have the label column\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower() or 'class' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col is None:\n",
    "            raise ValueError(\"No label column found in dataset\")\n",
    "        \n",
    "        # Select features and label\n",
    "        if len(selected_cols) < 20:  # Fallback if feature matching fails\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            selected_cols = [col for col in numeric_cols if col != label_col][:46]\n",
    "        \n",
    "        self.selected_features = selected_cols\n",
    "        self.feature_names = selected_cols\n",
    "        self.metadata['feature_groups'] = feature_to_group\n",
    "        self.metadata['num_features'] = len(selected_cols)\n",
    "        \n",
    "        return df[selected_cols + [label_col]]\n",
    "    \n",
    "    def encode_labels(self, y: pd.Series, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode labels to integers.\n",
    "        \n",
    "        Args:\n",
    "            y: Label series\n",
    "            fit: Whether to fit the encoder\n",
    "            \n",
    "        Returns:\n",
    "            Encoded labels\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            self.metadata['classes'] = self.label_encoder.classes_.tolist()\n",
    "            self.metadata['num_classes'] = len(self.label_encoder.classes_)\n",
    "            \n",
    "            # Calculate class weights for imbalanced learning\n",
    "            unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "            self.class_weights = len(y_encoded) / (len(unique) * counts)\n",
    "            self.metadata['class_distribution'] = dict(zip(\n",
    "                self.label_encoder.classes_,\n",
    "                counts.tolist()\n",
    "            ))\n",
    "        else:\n",
    "            y_encoded = self.label_encoder.transform(y)\n",
    "        \n",
    "        return y_encoded\n",
    "    \n",
    "    def scale_features(self, X: pd.DataFrame, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale features using StandardScaler.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            fit: Whether to fit the scaler\n",
    "            \n",
    "        Returns:\n",
    "            Scaled features\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Split data into train, validation, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature array\n",
    "            y: Label array\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train, val, test splits\n",
    "        \"\"\"\n",
    "        # First split: train+val vs test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size_adjusted = self.val_size / (1 - self.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=val_size_adjusted,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        splits = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        # Store split sizes in metadata\n",
    "        self.metadata['split_sizes'] = {\n",
    "            'train': len(X_train),\n",
    "            'val': len(X_val),\n",
    "            'test': len(X_test)\n",
    "        }\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def process(self, save: bool = True) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run the complete processing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            save: Whether to save processed data to disk\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed data splits\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        df = self.load_and_clean_data()\n",
    "        \n",
    "        print(\"Selecting features...\")\n",
    "        df = self.select_features(df)\n",
    "        \n",
    "        # Separate features and labels\n",
    "        label_col = df.columns[-1]\n",
    "        X = df.drop(columns=[label_col])\n",
    "        y = df[label_col]\n",
    "        \n",
    "        print(\"Encoding labels...\")\n",
    "        y_encoded = self.encode_labels(y, fit=True)\n",
    "        \n",
    "        print(\"Scaling features...\")\n",
    "        X_scaled = self.scale_features(X, fit=True)\n",
    "        \n",
    "        print(\"Splitting data...\")\n",
    "        splits = self.split_data(X_scaled, y_encoded)\n",
    "        \n",
    "        if save:\n",
    "            self.save_processed_data(splits)\n",
    "            self.save_preprocessors()\n",
    "            self.save_metadata()\n",
    "            self.save_combined_csv(splits)\n",
    "        \n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Train: {splits['X_train'].shape}, Val: {splits['X_val'].shape}, Test: {splits['X_test'].shape}\")\n",
    "        print(f\"Classes: {self.metadata['num_classes']}\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def save_combined_csv(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save combined feature-label CSV files for each split.\"\"\"\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            X = splits[f'X_{split}']\n",
    "            y = splits[f'y_{split}']\n",
    "            \n",
    "            # Create DataFrame with features and labels\n",
    "            df = pd.DataFrame(X, columns=self.feature_names)\n",
    "            df['label_encoded'] = y\n",
    "            df['label'] = self.label_encoder.inverse_transform(y)\n",
    "            \n",
    "            # Save combined CSV\n",
    "            df.to_csv(self.output_dir / f'{split}_data.csv', index=False)\n",
    "            print(f\"Saved {split}_data.csv with shape {df.shape}\")\n",
    "    \n",
    "    def save_processed_data(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save processed data splits to disk.\"\"\"\n",
    "        for name, data in splits.items():\n",
    "            np.save(self.output_dir / f'{name}.npy', data)\n",
    "    \n",
    "    def save_preprocessors(self):\n",
    "        \"\"\"Save fitted preprocessors.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            np.save(self.output_dir / 'class_weights.npy', self.class_weights)\n",
    "    \n",
    "    def save_metadata(self):\n",
    "        \"\"\"Save metadata as JSON.\"\"\"\n",
    "        metadata = self.metadata.copy()\n",
    "        metadata['feature_names'] = self.feature_names\n",
    "        metadata['selected_features'] = self.selected_features\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def load_preprocessors(self):\n",
    "        \"\"\"Load saved preprocessors for inference.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        if (self.output_dir / 'class_weights.npy').exists():\n",
    "            self.class_weights = np.load(self.output_dir / 'class_weights.npy')\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            self.feature_names = self.metadata.get('feature_names', [])\n",
    "            self.selected_features = self.metadata.get('selected_features', [])\n",
    "    \n",
    "    def transform_new_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Transform new data using fitted preprocessors.\n",
    "        \n",
    "        Args:\n",
    "            df: New data DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (scaled features, encoded labels or None)\n",
    "        \"\"\"\n",
    "        # Clean data\n",
    "        df = self.load_and_clean_data(df)\n",
    "        \n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Select features\n",
    "        selected_cols = []\n",
    "        for feat in self.selected_features:\n",
    "            matching_cols = [col for col in df.columns if feat in col or col in feat]\n",
    "            if matching_cols:\n",
    "                selected_cols.append(matching_cols[0])\n",
    "            else:\n",
    "                # Create missing column with zeros\n",
    "                df[feat] = 0\n",
    "                selected_cols.append(feat)\n",
    "        \n",
    "        X = df[selected_cols]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scale_features(X, fit=False)\n",
    "        \n",
    "        # Handle labels if present\n",
    "        y_encoded = None\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col:\n",
    "            y = df[label_col]\n",
    "            y_encoded = self.encode_labels(y, fit=False)\n",
    "        \n",
    "        return X_scaled, y_encoded\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = CICIoTDataPipeline(\n",
    "        data_path=\"/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Datasets/CIC_IoT/ciciot_processed_datasets_corrected/test_50_50.csv\",  # INSERT YOUR PATH HERE\n",
    "        output_dir=\"./processed_data\",\n",
    "        test_size=0.2,\n",
    "        val_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Process training data\n",
    "    splits = pipeline.process(save=True)\n",
    "    \n",
    "    # For inference on new data:\n",
    "    # pipeline.load_preprocessors()\n",
    "    # X_new, y_new = pipeline.transform_new_data(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14798fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/processed_data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418532db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "BenignTraffic              658916\n",
       "DDoS-SynonymousIP_Flood       752\n",
       "VulnerabilityScan             752\n",
       "SqlInjection                  752\n",
       "DDoS-UDP_Flood                752\n",
       "DDoS-ACK_Fragmentation        752\n",
       "DDoS-RSTFINFlood              752\n",
       "MITM-ArpSpoofing              752\n",
       "DDoS-HTTP_Flood               751\n",
       "DoS-TCP_Flood                 751\n",
       "Recon-OSScan                  751\n",
       "DDoS-SYN_Flood                751\n",
       "Recon-PingSweep               751\n",
       "Recon-PortScan                751\n",
       "DDoS-UDP_Fragmentation        751\n",
       "Backdoor_Malware              751\n",
       "DDoS-SlowLoris                751\n",
       "DDoS-PSHACK_Flood             751\n",
       "BrowserHijacking              751\n",
       "DoS-SYN_Flood                 751\n",
       "DictionaryBruteForce          751\n",
       "CommandInjection              751\n",
       "DDoS-TCP_Flood                751\n",
       "Uploading_Attack              751\n",
       "DoS-UDP_Flood                 751\n",
       "Mirai-greeth_flood            751\n",
       "Recon-HostDiscovery           751\n",
       "Mirai-greip_flood             751\n",
       "DDoS-ICMP_Fragmentation       751\n",
       "DDoS-ICMP_Flood               751\n",
       "Mirai-udpplain                751\n",
       "DNS_Spoofing                  751\n",
       "XSS                           751\n",
       "DoS-HTTP_Flood                751\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9dae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
