{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a8ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a6b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3868d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "\t'Benign_test.pcap.csv',\n",
    "\t'Benign_train.pcap.csv',\n",
    "\t'Recon-Ping_Sweep_train.pcap.csv',\n",
    "\t'Recon-Port_Scan_test.pcap.csv',\n",
    "\t'Recon-Port_Scan_train.pcap.csv',\n",
    "\t'Recon-VulScan_test.pcap.csv',\n",
    "\t'TCP_IP-DDoS-ICMP1_test.pcap.csv',\n",
    "\t'TCP_IP-DDoS-ICMP1_train.pcap.csv',\n",
    "\t'TCP_IP-DDoS-ICMP2_test.pcap.csv',\n",
    "\t'TCP_IP-DDoS-ICMP2_train.pcap.csv',\n",
    "\t'TCP_IP-DDoS-ICMP3_train.pcap.csv',\n",
    "\t'TCP_IP-DDoS-SYN_test.pcap.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9493653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign_test.pcap.csv exists.\n",
      "Benign_train.pcap.csv exists.\n",
      "Recon-Ping_Sweep_train.pcap.csv exists.\n",
      "Recon-Port_Scan_test.pcap.csv exists.\n",
      "Recon-Port_Scan_train.pcap.csv exists.\n",
      "Recon-VulScan_test.pcap.csv exists.\n",
      "TCP_IP-DDoS-ICMP1_test.pcap.csv exists.\n",
      "TCP_IP-DDoS-ICMP1_train.pcap.csv exists.\n",
      "TCP_IP-DDoS-ICMP2_test.pcap.csv exists.\n",
      "TCP_IP-DDoS-ICMP2_train.pcap.csv exists.\n",
      "TCP_IP-DDoS-ICMP3_train.pcap.csv exists.\n",
      "TCP_IP-DDoS-SYN_test.pcap.csv exists.\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    file_path = BASE_DIR / dataset\n",
    "    if file_path.exists():\n",
    "        print(f\"{dataset} exists.\")\n",
    "    else:\n",
    "        print(f\"{dataset} is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a3f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Benign_test.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Benign_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Recon-Ping_Sweep_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Recon-Port_Scan_test.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Recon-Port_Scan_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/Recon-VulScan_test.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-ICMP1_test.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-ICMP1_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-ICMP2_test.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-ICMP2_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-ICMP3_train.pcap.csv\n",
      "/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/TCP_IP-DDoS-SYN_test.pcap.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a list of file paths for each dataset and print them\n",
    "dataset_paths = []\n",
    "for i in range(len(datasets)):\n",
    "    path = BASE_DIR / datasets[i]\n",
    "    dataset_paths.append(path)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b98b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv(dataset_paths[0])\n",
    "df_2 = pd.read_csv(dataset_paths[1])\n",
    "df_3 = pd.read_csv(dataset_paths[2])\n",
    "df_4 = pd.read_csv(dataset_paths[3])\n",
    "df_5 = pd.read_csv(dataset_paths[4])\n",
    "df_6 = pd.read_csv(dataset_paths[5])\n",
    "df_7 = pd.read_csv(dataset_paths[6])\n",
    "df_8 = pd.read_csv(dataset_paths[7])\n",
    "df_9 = pd.read_csv(dataset_paths[8])\n",
    "df_10 = pd.read_csv(dataset_paths[9])\n",
    "df_11 = pd.read_csv(dataset_paths[10])\n",
    "df_12 = pd.read_csv(dataset_paths[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2d919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d054fbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>ack_flag_number</th>\n",
       "      <th>ece_flag_number</th>\n",
       "      <th>cwr_flag_number</th>\n",
       "      <th>ack_count</th>\n",
       "      <th>syn_count</th>\n",
       "      <th>fin_count</th>\n",
       "      <th>rst_count</th>\n",
       "      <th>HTTP</th>\n",
       "      <th>HTTPS</th>\n",
       "      <th>DNS</th>\n",
       "      <th>Telnet</th>\n",
       "      <th>SMTP</th>\n",
       "      <th>SSH</th>\n",
       "      <th>IRC</th>\n",
       "      <th>TCP</th>\n",
       "      <th>UDP</th>\n",
       "      <th>DHCP</th>\n",
       "      <th>ARP</th>\n",
       "      <th>ICMP</th>\n",
       "      <th>IGMP</th>\n",
       "      <th>IPv</th>\n",
       "      <th>LLC</th>\n",
       "      <th>Tot sum</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>AVG</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>83.1</td>\n",
       "      <td>39123.054540</td>\n",
       "      <td>39123.054540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>616.4</td>\n",
       "      <td>85.8</td>\n",
       "      <td>246.0</td>\n",
       "      <td>130.133651</td>\n",
       "      <td>58.513912</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.694703e+08</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.954786</td>\n",
       "      <td>82.751168</td>\n",
       "      <td>3899.571692</td>\n",
       "      <td>0.9</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>44.8</td>\n",
       "      <td>72928.095973</td>\n",
       "      <td>72928.095973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1377.6</td>\n",
       "      <td>58.8</td>\n",
       "      <td>246.0</td>\n",
       "      <td>89.688165</td>\n",
       "      <td>44.556792</td>\n",
       "      <td>64.4</td>\n",
       "      <td>1.694703e+08</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.405243</td>\n",
       "      <td>63.022662</td>\n",
       "      <td>1993.712535</td>\n",
       "      <td>1.0</td>\n",
       "      <td>244.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52552.111857</td>\n",
       "      <td>52552.111857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>766.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>494.2</td>\n",
       "      <td>113.092857</td>\n",
       "      <td>148.485766</td>\n",
       "      <td>152.1</td>\n",
       "      <td>9.108920e-02</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14.407632</td>\n",
       "      <td>209.990584</td>\n",
       "      <td>42487.982728</td>\n",
       "      <td>0.9</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>24502.562704</td>\n",
       "      <td>24502.562704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1953.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>127.680349</td>\n",
       "      <td>214.732903</td>\n",
       "      <td>80.2</td>\n",
       "      <td>1.694703e+08</td>\n",
       "      <td>13.5</td>\n",
       "      <td>15.986909</td>\n",
       "      <td>304.317403</td>\n",
       "      <td>46658.604609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>244.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6263.156480</td>\n",
       "      <td>6263.156480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>767.9</td>\n",
       "      <td>68.5</td>\n",
       "      <td>427.0</td>\n",
       "      <td>119.869921</td>\n",
       "      <td>116.233361</td>\n",
       "      <td>162.5</td>\n",
       "      <td>9.976871e-02</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.169114</td>\n",
       "      <td>164.378795</td>\n",
       "      <td>30786.754577</td>\n",
       "      <td>0.9</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Header_Length  Protocol Type  Duration          Rate         Srate  Drate  \\\n",
       "0          138.2            7.1      83.1  39123.054540  39123.054540    0.0   \n",
       "1           86.0            4.2      44.8  72928.095973  72928.095973    0.0   \n",
       "2          171.4            4.1      32.0  52552.111857  52552.111857    0.0   \n",
       "3          227.8            6.0      64.0  24502.562704  24502.562704    0.0   \n",
       "4          347.6            7.1      64.0   6263.156480   6263.156480    0.0   \n",
       "\n",
       "   fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
       "0              0.0              0.0              0.0              0.5   \n",
       "1              0.0              0.0              0.0              0.3   \n",
       "2              0.0              0.0              0.0              0.2   \n",
       "3              0.0              0.0              0.0              0.5   \n",
       "4              0.0              0.0              0.0              0.5   \n",
       "\n",
       "   ack_flag_number  ece_flag_number  cwr_flag_number  ack_count  syn_count  \\\n",
       "0              0.9              0.0              0.0        0.0        0.0   \n",
       "1              0.7              0.0              0.0        0.0        0.0   \n",
       "2              0.4              0.0              0.0        0.0        0.0   \n",
       "3              1.0              0.0              0.0        0.0        0.0   \n",
       "4              0.9              0.0              0.0        0.0        0.0   \n",
       "\n",
       "   fin_count  rst_count  HTTP  HTTPS  DNS  Telnet  SMTP  SSH  IRC  TCP  UDP  \\\n",
       "0        0.0        1.3   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.9  0.1   \n",
       "1        0.0        1.1   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.7  0.0   \n",
       "2        0.0        1.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.4  0.1   \n",
       "3        0.0        2.7   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
       "4        0.0        3.3   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.9  0.1   \n",
       "\n",
       "   DHCP  ARP  ICMP  IGMP  IPv  LLC  Tot sum   Min    Max         AVG  \\\n",
       "0   0.0  0.0   0.0   0.0  1.0  1.0    616.4  85.8  246.0  130.133651   \n",
       "1   0.0  0.3   0.0   0.0  0.7  0.7   1377.6  58.8  246.0   89.688165   \n",
       "2   0.0  0.5   0.0   0.0  0.5  0.5    766.2  42.0  494.2  113.092857   \n",
       "3   0.0  0.0   0.0   0.0  1.0  1.0   1953.2  42.0  932.0  127.680349   \n",
       "4   0.0  0.0   0.0   0.0  1.0  1.0    767.9  68.5  427.0  119.869921   \n",
       "\n",
       "          Std  Tot size           IAT  Number   Magnitue      Radius  \\\n",
       "0   58.513912      99.0  1.694703e+08     5.5  15.954786   82.751168   \n",
       "1   44.556792      64.4  1.694703e+08    13.5  13.405243   63.022662   \n",
       "2  148.485766     152.1  9.108920e-02     5.5  14.407632  209.990584   \n",
       "3  214.732903      80.2  1.694703e+08    13.5  15.986909  304.317403   \n",
       "4  116.233361     162.5  9.976871e-02     5.5  15.169114  164.378795   \n",
       "\n",
       "     Covariance  Variance  Weight  \n",
       "0   3899.571692       0.9    38.5  \n",
       "1   1993.712535       1.0   244.6  \n",
       "2  42487.982728       0.9    38.5  \n",
       "3  46658.604609       1.0   244.6  \n",
       "4  30786.754577       0.9    38.5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30d5f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37607 entries, 0 to 37606\n",
      "Data columns (total 45 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Header_Length    37607 non-null  float64\n",
      " 1   Protocol Type    37607 non-null  float64\n",
      " 2   Duration         37607 non-null  float64\n",
      " 3   Rate             37607 non-null  float64\n",
      " 4   Srate            37607 non-null  float64\n",
      " 5   Drate            37607 non-null  float64\n",
      " 6   fin_flag_number  37607 non-null  float64\n",
      " 7   syn_flag_number  37607 non-null  float64\n",
      " 8   rst_flag_number  37607 non-null  float64\n",
      " 9   psh_flag_number  37607 non-null  float64\n",
      " 10  ack_flag_number  37607 non-null  float64\n",
      " 11  ece_flag_number  37607 non-null  float64\n",
      " 12  cwr_flag_number  37607 non-null  float64\n",
      " 13  ack_count        37607 non-null  float64\n",
      " 14  syn_count        37607 non-null  float64\n",
      " 15  fin_count        37607 non-null  float64\n",
      " 16  rst_count        37607 non-null  float64\n",
      " 17  HTTP             37607 non-null  float64\n",
      " 18  HTTPS            37607 non-null  float64\n",
      " 19  DNS              37607 non-null  float64\n",
      " 20  Telnet           37607 non-null  float64\n",
      " 21  SMTP             37607 non-null  float64\n",
      " 22  SSH              37607 non-null  float64\n",
      " 23  IRC              37607 non-null  float64\n",
      " 24  TCP              37607 non-null  float64\n",
      " 25  UDP              37607 non-null  float64\n",
      " 26  DHCP             37607 non-null  float64\n",
      " 27  ARP              37607 non-null  float64\n",
      " 28  ICMP             37607 non-null  float64\n",
      " 29  IGMP             37607 non-null  float64\n",
      " 30  IPv              37607 non-null  float64\n",
      " 31  LLC              37607 non-null  float64\n",
      " 32  Tot sum          37607 non-null  float64\n",
      " 33  Min              37607 non-null  float64\n",
      " 34  Max              37607 non-null  float64\n",
      " 35  AVG              37607 non-null  float64\n",
      " 36  Std              37607 non-null  float64\n",
      " 37  Tot size         37607 non-null  float64\n",
      " 38  IAT              37607 non-null  float64\n",
      " 39  Number           37607 non-null  float64\n",
      " 40  Magnitue         37607 non-null  float64\n",
      " 41  Radius           37607 non-null  float64\n",
      " 42  Covariance       37607 non-null  float64\n",
      " 43  Variance         37607 non-null  float64\n",
      " 44  Weight           37607 non-null  float64\n",
      "dtypes: float64(45)\n",
      "memory usage: 12.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81be51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['label'] = 'Benign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81700149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37607 entries, 0 to 37606\n",
      "Data columns (total 45 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Header_Length    37607 non-null  float64\n",
      " 1   Protocol Type    37607 non-null  float64\n",
      " 2   Duration         37607 non-null  float64\n",
      " 3   Rate             37607 non-null  float64\n",
      " 4   Srate            37607 non-null  float64\n",
      " 5   Drate            37607 non-null  float64\n",
      " 6   fin_flag_number  37607 non-null  float64\n",
      " 7   syn_flag_number  37607 non-null  float64\n",
      " 8   rst_flag_number  37607 non-null  float64\n",
      " 9   psh_flag_number  37607 non-null  float64\n",
      " 10  ack_flag_number  37607 non-null  float64\n",
      " 11  ece_flag_number  37607 non-null  float64\n",
      " 12  cwr_flag_number  37607 non-null  float64\n",
      " 13  ack_count        37607 non-null  float64\n",
      " 14  syn_count        37607 non-null  float64\n",
      " 15  fin_count        37607 non-null  float64\n",
      " 16  rst_count        37607 non-null  float64\n",
      " 17  HTTP             37607 non-null  float64\n",
      " 18  HTTPS            37607 non-null  float64\n",
      " 19  DNS              37607 non-null  float64\n",
      " 20  Telnet           37607 non-null  float64\n",
      " 21  SMTP             37607 non-null  float64\n",
      " 22  SSH              37607 non-null  float64\n",
      " 23  IRC              37607 non-null  float64\n",
      " 24  TCP              37607 non-null  float64\n",
      " 25  UDP              37607 non-null  float64\n",
      " 26  DHCP             37607 non-null  float64\n",
      " 27  ARP              37607 non-null  float64\n",
      " 28  ICMP             37607 non-null  float64\n",
      " 29  IPv              37607 non-null  float64\n",
      " 30  LLC              37607 non-null  float64\n",
      " 31  Tot sum          37607 non-null  float64\n",
      " 32  Min              37607 non-null  float64\n",
      " 33  Max              37607 non-null  float64\n",
      " 34  AVG              37607 non-null  float64\n",
      " 35  Std              37607 non-null  float64\n",
      " 36  Tot size         37607 non-null  float64\n",
      " 37  IAT              37607 non-null  float64\n",
      " 38  Number           37607 non-null  float64\n",
      " 39  Magnitue         37607 non-null  float64\n",
      " 40  Radius           37607 non-null  float64\n",
      " 41  Covariance       37607 non-null  float64\n",
      " 42  Variance         37607 non-null  float64\n",
      " 43  Weight           37607 non-null  float64\n",
      " 44  label            37607 non-null  object \n",
      "dtypes: float64(44), object(1)\n",
      "memory usage: 12.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954b4fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(37607)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd276193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.drop(columns=['IGMP'], inplace=True)\n",
    "df_3.drop(columns=['IGMP'], inplace=True)\n",
    "df_4.drop(columns=['IGMP'], inplace=True)\n",
    "df_5.drop(columns=['IGMP'], inplace=True)\n",
    "df_6.drop(columns=['IGMP'], inplace=True)\n",
    "df_7.drop(columns=['IGMP'], inplace=True)\n",
    "df_8.drop(columns=['IGMP'], inplace=True)\n",
    "df_9.drop(columns=['IGMP'], inplace=True)\n",
    "df_10.drop(columns=['IGMP'], inplace=True)\n",
    "df_11.drop(columns=['IGMP'], inplace=True)\n",
    "df_12.drop(columns=['IGMP'], inplace=True)\n",
    "df_2['label'] = 'Benign'\n",
    "df_3['label'] = 'Recon-Ping_Sweep'\n",
    "df_4['label'] = 'Recon-Port_Scan'\n",
    "df_5['label'] = 'Recon-Port_Scan'\n",
    "df_6['label'] = 'Recon-VulScan'\n",
    "df_7['label'] = 'TCP_IP-DDoS-ICMP1'\n",
    "df_8['label'] = 'TCP_IP-DDoS-ICMP1'\n",
    "df_9['label'] = 'TCP_IP-DDoS-ICMP2'\n",
    "df_10['label'] = 'TCP_IP-DDoS-ICMP2'\n",
    "df_11['label'] = 'TCP_IP-DDoS-ICMP3'\n",
    "df_12['label'] = 'TCP_IP-DDoS-SYN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e829401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8debed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1440278 entries, 0 to 1440277\n",
      "Data columns (total 45 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   Header_Length    1440278 non-null  float64\n",
      " 1   Protocol Type    1440278 non-null  float64\n",
      " 2   Duration         1440278 non-null  float64\n",
      " 3   Rate             1440278 non-null  float64\n",
      " 4   Srate            1440278 non-null  float64\n",
      " 5   Drate            1440278 non-null  float64\n",
      " 6   fin_flag_number  1440278 non-null  float64\n",
      " 7   syn_flag_number  1440278 non-null  float64\n",
      " 8   rst_flag_number  1440278 non-null  float64\n",
      " 9   psh_flag_number  1440278 non-null  float64\n",
      " 10  ack_flag_number  1440278 non-null  float64\n",
      " 11  ece_flag_number  1440278 non-null  float64\n",
      " 12  cwr_flag_number  1440278 non-null  float64\n",
      " 13  ack_count        1440278 non-null  float64\n",
      " 14  syn_count        1440278 non-null  float64\n",
      " 15  fin_count        1440278 non-null  float64\n",
      " 16  rst_count        1440278 non-null  float64\n",
      " 17  HTTP             1440278 non-null  float64\n",
      " 18  HTTPS            1440278 non-null  float64\n",
      " 19  DNS              1440278 non-null  float64\n",
      " 20  Telnet           1440278 non-null  float64\n",
      " 21  SMTP             1440278 non-null  float64\n",
      " 22  SSH              1440278 non-null  float64\n",
      " 23  IRC              1440278 non-null  float64\n",
      " 24  TCP              1440278 non-null  float64\n",
      " 25  UDP              1440278 non-null  float64\n",
      " 26  DHCP             1440278 non-null  float64\n",
      " 27  ARP              1440278 non-null  float64\n",
      " 28  ICMP             1440278 non-null  float64\n",
      " 29  IPv              1440278 non-null  float64\n",
      " 30  LLC              1440278 non-null  float64\n",
      " 31  Tot sum          1440278 non-null  float64\n",
      " 32  Min              1440278 non-null  float64\n",
      " 33  Max              1440278 non-null  float64\n",
      " 34  AVG              1440278 non-null  float64\n",
      " 35  Std              1440278 non-null  float64\n",
      " 36  Tot size         1440278 non-null  float64\n",
      " 37  IAT              1440278 non-null  float64\n",
      " 38  Number           1440278 non-null  float64\n",
      " 39  Magnitue         1440278 non-null  float64\n",
      " 40  Radius           1440278 non-null  float64\n",
      " 41  Covariance       1440278 non-null  float64\n",
      " 42  Variance         1440278 non-null  float64\n",
      " 43  Weight           1440278 non-null  float64\n",
      " 44  label            1440278 non-null  object \n",
      "dtypes: float64(44), object(1)\n",
      "memory usage: 494.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ca8dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "TCP_IP-DDoS-ICMP2    390510\n",
       "TCP_IP-DDoS-ICMP1    348945\n",
       "Benign               230339\n",
       "TCP_IP-DDoS-ICMP3    189710\n",
       "TCP_IP-DDoS-SYN      172397\n",
       "Recon-Port_Scan      106603\n",
       "Recon-VulScan          1034\n",
       "Recon-Ping_Sweep        740\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc8ab06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Selecting features...\n",
      "Encoding labels...\n",
      "Scaling features...\n",
      "Splitting data...\n",
      "Saved train_data.csv with shape (860652, 42)\n",
      "Saved val_data.csv with shape (286884, 42)\n",
      "Saved test_data.csv with shape (286884, 42)\n",
      "Processing complete!\n",
      "Train: (860652, 40), Val: (286884, 40), Test: (286884, 40)\n",
      "Classes: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CICIoTDataPipeline:\n",
    "    \"\"\"Data processing pipeline for CIC-IoT-2023 dataset for GRIFFIN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path: str,\n",
    "                 output_dir: str = '.',\n",
    "                 test_size: float = 0.2,\n",
    "                 val_size: float = 0.2,\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the raw CIC-IoT dataset CSV file\n",
    "            output_dir: Directory to save processed data\n",
    "            test_size: Proportion of data for testing\n",
    "            val_size: Proportion of training data for validation\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Feature groups for GRIFFIN (Protocol-Aware Group Gates)\n",
    "        self.feature_groups = {\n",
    "            'packet_stats': [\n",
    "                'Total Fwd Packets', 'Total Backward Packets',\n",
    "                'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "                'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "                'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
    "                'Bwd Packet Length Mean', 'Bwd Packet Length Std'\n",
    "            ],\n",
    "            'time_features': [\n",
    "                'Flow IAT Mean', 'Flow IAT Std',\n",
    "                'Flow IAT Max', 'Flow IAT Min',\n",
    "                'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
    "                'Bwd IAT Total', 'Bwd IAT Mean'\n",
    "            ],\n",
    "            'flow_rates': [\n",
    "                'Flow Bytes/s', 'Flow Packets/s',\n",
    "                'Fwd Packets/s', 'Bwd Packets/s',\n",
    "                'Packet Length Mean', 'Packet Length Std',\n",
    "                'Packet Length Variance', 'Down/Up Ratio',\n",
    "                'Average Packet Size', 'Fwd Segment Size Avg'\n",
    "            ],\n",
    "            'tcp_flags': [\n",
    "                'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count',\n",
    "                'PSH Flag Count', 'ACK Flag Count',\n",
    "                'CWE Flag Count', 'ECE Flag Count',\n",
    "                'Fwd PSH Flags', 'Bwd PSH Flags'\n",
    "            ],\n",
    "            'protocol_info': [\n",
    "                'Protocol', 'Source Port', 'Destination Port',\n",
    "                'Init Win bytes forward', 'Init Win bytes backward',\n",
    "                'Active Mean', 'Active Std', 'Active Max',\n",
    "                'Idle Mean', 'Idle Std'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.selected_features = None\n",
    "        self.feature_names = None\n",
    "        self.class_weights = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_and_clean_data(self, df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and perform initial cleaning of the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: Optional DataFrame if data is already loaded\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        \n",
    "        # Store original shape\n",
    "        original_shape = df.shape\n",
    "        \n",
    "        # Handle column names (remove spaces, convert to lowercase)\n",
    "        df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('/', '_')\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Handle infinity values\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Clip extreme values\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].clip(-1e10, 1e10)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "        \n",
    "        # Remove constant features (variance < 1e-6)\n",
    "        constant_features = []\n",
    "        for col in numeric_columns:\n",
    "            if df[col].var() < 1e-6:\n",
    "                constant_features.append(col)\n",
    "        \n",
    "        if constant_features:\n",
    "            df = df.drop(columns=constant_features)\n",
    "        \n",
    "        self.metadata['original_shape'] = original_shape\n",
    "        self.metadata['cleaned_shape'] = df.shape\n",
    "        self.metadata['removed_constants'] = constant_features\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def select_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Select and organize features according to GRIFFIN groups.\n",
    "        \n",
    "        Args:\n",
    "            df: Cleaned DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with selected features\n",
    "        \"\"\"\n",
    "        # Normalize column names for matching\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Create mapping for feature groups (normalized names)\n",
    "        normalized_groups = {}\n",
    "        for group_name, features in self.feature_groups.items():\n",
    "            normalized_groups[group_name] = [\n",
    "                f.lower().replace(' ', '').replace('/', '').replace('_', '')\n",
    "                for f in features\n",
    "            ]\n",
    "        \n",
    "        # Find matching columns\n",
    "        selected_cols = []\n",
    "        feature_to_group = {}\n",
    "        \n",
    "        for group_name, norm_features in normalized_groups.items():\n",
    "            for norm_feat in norm_features:\n",
    "                matching_cols = [col for col in df.columns \n",
    "                               if norm_feat in col or col in norm_feat]\n",
    "                if matching_cols:\n",
    "                    col = matching_cols[0]  # Take first match\n",
    "                    if col not in selected_cols and col != 'label':\n",
    "                        selected_cols.append(col)\n",
    "                        feature_to_group[col] = group_name\n",
    "        \n",
    "        # Ensure we have the label column\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower() or 'class' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col is None:\n",
    "            raise ValueError(\"No label column found in dataset\")\n",
    "        \n",
    "        # Select features and label\n",
    "        if len(selected_cols) < 20:  # Fallback if feature matching fails\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            selected_cols = [col for col in numeric_cols if col != label_col][:46]\n",
    "        \n",
    "        self.selected_features = selected_cols\n",
    "        self.feature_names = selected_cols\n",
    "        self.metadata['feature_groups'] = feature_to_group\n",
    "        self.metadata['num_features'] = len(selected_cols)\n",
    "        \n",
    "        return df[selected_cols + [label_col]]\n",
    "    \n",
    "    def encode_labels(self, y: pd.Series, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode labels to integers.\n",
    "        \n",
    "        Args:\n",
    "            y: Label series\n",
    "            fit: Whether to fit the encoder\n",
    "            \n",
    "        Returns:\n",
    "            Encoded labels\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            self.metadata['classes'] = self.label_encoder.classes_.tolist()\n",
    "            self.metadata['num_classes'] = len(self.label_encoder.classes_)\n",
    "            \n",
    "            # Calculate class weights for imbalanced learning\n",
    "            unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "            self.class_weights = len(y_encoded) / (len(unique) * counts)\n",
    "            self.metadata['class_distribution'] = dict(zip(\n",
    "                self.label_encoder.classes_,\n",
    "                counts.tolist()\n",
    "            ))\n",
    "        else:\n",
    "            y_encoded = self.label_encoder.transform(y)\n",
    "        \n",
    "        return y_encoded\n",
    "    \n",
    "    def scale_features(self, X: pd.DataFrame, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale features using StandardScaler.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            fit: Whether to fit the scaler\n",
    "            \n",
    "        Returns:\n",
    "            Scaled features\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def split_data(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Split data into train, validation, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature array\n",
    "            y: Label array\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train, val, test splits\n",
    "        \"\"\"\n",
    "        # First split: train+val vs test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size_adjusted = self.val_size / (1 - self.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=val_size_adjusted,\n",
    "            random_state=self.random_state,\n",
    "            stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        splits = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        # Store split sizes in metadata\n",
    "        self.metadata['split_sizes'] = {\n",
    "            'train': len(X_train),\n",
    "            'val': len(X_val),\n",
    "            'test': len(X_test)\n",
    "        }\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def process(self, save: bool = True) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run the complete processing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            save: Whether to save processed data to disk\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed data splits\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        df = self.load_and_clean_data()\n",
    "        \n",
    "        print(\"Selecting features...\")\n",
    "        df = self.select_features(df)\n",
    "        \n",
    "        # Separate features and labels\n",
    "        label_col = df.columns[-1]\n",
    "        X = df.drop(columns=[label_col])\n",
    "        y = df[label_col]\n",
    "        \n",
    "        print(\"Encoding labels...\")\n",
    "        y_encoded = self.encode_labels(y, fit=True)\n",
    "        \n",
    "        print(\"Scaling features...\")\n",
    "        X_scaled = self.scale_features(X, fit=True)\n",
    "        \n",
    "        print(\"Splitting data...\")\n",
    "        splits = self.split_data(X_scaled, y_encoded)\n",
    "        \n",
    "        if save:\n",
    "            self.save_processed_data(splits)\n",
    "            self.save_preprocessors()\n",
    "            self.save_metadata()\n",
    "            self.save_combined_csv(splits)\n",
    "        \n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Train: {splits['X_train'].shape}, Val: {splits['X_val'].shape}, Test: {splits['X_test'].shape}\")\n",
    "        print(f\"Classes: {self.metadata['num_classes']}\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def save_combined_csv(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save combined feature-label CSV files for each split.\"\"\"\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            X = splits[f'X_{split}']\n",
    "            y = splits[f'y_{split}']\n",
    "            \n",
    "            # Create DataFrame with features and labels\n",
    "            df = pd.DataFrame(X, columns=self.feature_names)\n",
    "            df['label_encoded'] = y\n",
    "            df['label'] = self.label_encoder.inverse_transform(y)\n",
    "            \n",
    "            # Save combined CSV\n",
    "            df.to_csv(self.output_dir / f'{split}_data.csv', index=False)\n",
    "            print(f\"Saved {split}_data.csv with shape {df.shape}\")\n",
    "    \n",
    "    def save_processed_data(self, splits: Dict[str, np.ndarray]):\n",
    "        \"\"\"Save processed data splits to disk.\"\"\"\n",
    "        for name, data in splits.items():\n",
    "            np.save(self.output_dir / f'{name}.npy', data)\n",
    "    \n",
    "    def save_preprocessors(self):\n",
    "        \"\"\"Save fitted preprocessors.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            np.save(self.output_dir / 'class_weights.npy', self.class_weights)\n",
    "    \n",
    "    def save_metadata(self):\n",
    "        \"\"\"Save metadata as JSON.\"\"\"\n",
    "        metadata = self.metadata.copy()\n",
    "        metadata['feature_names'] = self.feature_names\n",
    "        metadata['selected_features'] = self.selected_features\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def load_preprocessors(self):\n",
    "        \"\"\"Load saved preprocessors for inference.\"\"\"\n",
    "        with open(self.output_dir / 'scaler.pkl', 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        with open(self.output_dir / 'label_encoder.pkl', 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        if (self.output_dir / 'class_weights.npy').exists():\n",
    "            self.class_weights = np.load(self.output_dir / 'class_weights.npy')\n",
    "        \n",
    "        with open(self.output_dir / 'metadata.json', 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            self.feature_names = self.metadata.get('feature_names', [])\n",
    "            self.selected_features = self.metadata.get('selected_features', [])\n",
    "    \n",
    "    def transform_new_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Transform new data using fitted preprocessors.\n",
    "        \n",
    "        Args:\n",
    "            df: New data DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (scaled features, encoded labels or None)\n",
    "        \"\"\"\n",
    "        # Clean data\n",
    "        df = self.load_and_clean_data(df)\n",
    "        \n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.lower().str.replace('_', '').str.replace(' ', '')\n",
    "        \n",
    "        # Select features\n",
    "        selected_cols = []\n",
    "        for feat in self.selected_features:\n",
    "            matching_cols = [col for col in df.columns if feat in col or col in feat]\n",
    "            if matching_cols:\n",
    "                selected_cols.append(matching_cols[0])\n",
    "            else:\n",
    "                # Create missing column with zeros\n",
    "                df[feat] = 0\n",
    "                selected_cols.append(feat)\n",
    "        \n",
    "        X = df[selected_cols]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scale_features(X, fit=False)\n",
    "        \n",
    "        # Handle labels if present\n",
    "        y_encoded = None\n",
    "        label_col = None\n",
    "        for col in df.columns:\n",
    "            if 'label' in col.lower() or 'attack' in col.lower():\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        if label_col:\n",
    "            y = df[label_col]\n",
    "            y_encoded = self.encode_labels(y, fit=False)\n",
    "        \n",
    "        return X_scaled, y_encoded\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = CICIoTDataPipeline(\n",
    "        data_path=\"/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/combined_data.csv\",  # INSERT YOUR PATH HERE\n",
    "        output_dir=\"/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/processes_2\",\n",
    "        test_size=0.2,\n",
    "        val_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Process training data\n",
    "    splits = pipeline.process(save=True)\n",
    "    \n",
    "    # For inference on new data:\n",
    "    # pipeline.load_preprocessors()\n",
    "    # X_new, y_new = pipeline.transform_new_data(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2563eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/fab3/btech/2022/siddhant.gond22b@iiitg.ac.in/Research_Internship_Under_Dr_Rakesh_Matam/Project_2/data/raw/combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fc300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
